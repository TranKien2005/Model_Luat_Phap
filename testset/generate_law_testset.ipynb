{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f6c331",
   "metadata": {},
   "source": [
    "C√†i ƒë·∫∑t th∆∞ vi·ªán google-generativeai ƒë·ªÉ s·ª≠ d·ª•ng API Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c0b280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-generativeai) (2.27.0)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-generativeai) (2.185.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-generativeai) (2.36.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-generativeai) (5.29.4)\n",
      "Requirement already satisfied: pydantic in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-generativeai) (2.11.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (1.76.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc44439c",
   "metadata": {},
   "source": [
    "Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt v√† c·∫•u h√¨nh API key cho Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53122a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ kh·ªüi t·∫°o m√¥ h√¨nh Gemini\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import google.generativeai as genai\n",
    "import re\n",
    "\n",
    "GOOGLE_API_KEY = \"AIzaSyD5NLgqsjm_sjTRkgh6Ckzv_9Li5B8k264\"\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "print(\"ƒê√£ kh·ªüi t·∫°o m√¥ h√¨nh Gemini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb3b795",
   "metadata": {},
   "source": [
    "ƒê·ªãnh nghƒ©a c√°c h√†m ƒë·ªÉ t·∫£i d·ªØ li·ªáu lu·∫≠t t·ª´ JSON, x√¢y d·ª±ng prompt cho Gemini v√† g·ªçi API Gemini ƒë·ªÉ sinh d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb95e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_law_document(filepath: str) -> str:\n",
    "    \"\"\"T·∫£i n·ªôi dung vƒÉn b·∫£n lu·∫≠t t·ª´ file JSON v√† parse th√†nh string ƒë·∫ßy ƒë·ªß.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        law_text = ''\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata = data.get('metadata', {})\n",
    "        law_text += f\"Metadata: Law ID {metadata.get('law_id')}, Version {metadata.get('version_id')}, Status {metadata.get('status')}, Last Updated {metadata.get('last_updated')}\\n\\n\"\n",
    "        \n",
    "        # Extract main law\n",
    "        law = data['content'].get('law', {})\n",
    "        law_text += f\"Law Type: {law.get('type')}\\nIssuer: {law.get('issuer')}\\nTitle: {law.get('title')}\\nSource URL: {law.get('source_url')}\\nPromulgation Date: {law.get('promulgation_date')}\\nEffective Date: {law.get('effective_date')}\\n\\n\"\n",
    "        \n",
    "        for chapter in law.get('structure', []):\n",
    "            law_text += f\"{chapter['type'].capitalize()} {chapter['number']}: {chapter['title']}\\n\"\n",
    "            for article in chapter.get('articles', []):\n",
    "                law_text += f\"Article {article['number']}: {article['title']}\\n\"\n",
    "                if article['text']:\n",
    "                    law_text += f\"{article['text']}\\n\"\n",
    "                for clause in article.get('clauses', []):\n",
    "                    law_text += f\"Clause {clause['number']}: {clause['text']}\\n\"\n",
    "                law_text += '\\n'\n",
    "        \n",
    "        # Extract related documents\n",
    "        for rel_doc in data['content'].get('related_documents', []):\n",
    "            law_text += f\"\\nRelated Document: {rel_doc.get('title')}\\nType: {rel_doc.get('type')}\\nIssuer: {rel_doc.get('issuer')}\\nPromulgation Date: {rel_doc.get('promulgation_date')}\\nEffective Date: {rel_doc.get('effective_date')}\\nRelationship: {rel_doc.get('relationship')}\\n\\n\"\n",
    "            for struct in rel_doc.get('structure', []):\n",
    "                law_text += f\"{struct['type'].capitalize()} {struct['number']}: {struct['title']}\\n\"\n",
    "                for article in struct.get('articles', []):\n",
    "                    law_text += f\"Article {article['number']}: {article['title']}\\n\"\n",
    "                    if article.get('text'):\n",
    "                        law_text += f\"{article['text']}\\n\"\n",
    "                    for clause in article.get('clauses', []):\n",
    "                        law_text += f\"Clause {clause['number']}: {clause['text']}\\n\"\n",
    "                    law_text += '\\n'\n",
    "        \n",
    "        return law_text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"L·ªñI: Kh√¥ng t√¨m th·∫•y file ngu·ªìn: {filepath}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªñI khi ƒë·ªçc file: {e}\")\n",
    "        return None\n",
    "\n",
    "def build_generation_prompt(topic: str, law_text: str, num_questions: int) -> str:\n",
    "    \"\"\"\n",
    "    Sinh t·∫≠p d·ªØ li·ªáu ki·ªÉm th·ª≠ (testset) ph√°p l√Ω d·ª±a tr√™n vƒÉn b·∫£n lu·∫≠t Vi·ªát Nam.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "B·∫°n l√† **tr·ª£ l√Ω ph√°p l√Ω**, ƒë∆∞·ª£c giao sinh t·∫≠p d·ªØ li·ªáu ki·ªÉm th·ª≠ (test) ch·∫•t l∆∞·ª£ng cao \n",
    "ƒë·ªÉ ƒë√°nh gi√° m√¥ h√¨nh LLM v·ªÅ nƒÉng l·ª±c **hi·ªÉu v√† tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn lu·∫≠t gi√°o d·ª•c**.\n",
    "Ngu·ªìn: Lu·∫≠t Gi√°o d·ª•c 2019 v√† c√°c vƒÉn b·∫£n li√™n quan.\n",
    "\n",
    "## ƒê·∫ßu v√†o: ‚Ä¶\n",
    "\n",
    "* T·∫≠p ch·ªß ƒë·ªÅ d·ª± ki·∫øn cho c√¢u:\n",
    "{topic} (b√°m s√°t n·ªôi dung JSON)\n",
    "\n",
    "## B·ªëi c·∫£nh h·ªôi tho·∫°i:\n",
    "* Khi h·ªèi: b·∫°n l√† c√¥ng d√¢n, h·ªçc sinh, ph·ª• huynh ho·∫∑c ng∆∞·ªùi quan t√¢m ƒë·∫øn gi√°o d·ª•c.\n",
    "* Khi tr·∫£ l·ªùi: b·∫°n l√† c√°n b·ªô t∆∞ v·∫•n ph√°p l√Ω thu·ªôc B·ªô Gi√°o d·ª•c.\n",
    "\n",
    "## Nhi·ªám v·ª•:\n",
    "Sinh d·ªØ li·ªáu g·ªìm c√°c c·∫∑p c√¢u h·ªèi - tr·∫£ l·ªùi d·ª±a theo n·ªôi dung lu·∫≠t. Sinh {num_questions} c√¢u h·ªèi thu·ªôc ch·ªß ƒë·ªÅ ƒë√≥. C√°c y√™u c·∫ßu chi ti·∫øt h∆°n c√≥ ·ªü d∆∞·ªõi ƒë√¢y.\n",
    "\n",
    "### B∆∞·ªõc 1. Sinh c√¢u h·ªèi:\n",
    "* B√°m s√°t ho·∫∑c m·ªü r·ªông t·ª´ n·ªôi dung lu·∫≠t. C√≥ th·ªÉ h·ªèi v·ªÅ: m·ª•c ti√™u, h·ªá th·ªëng, ch∆∞∆°ng tr√¨nh, quy·ªÅn nghƒ©a v·ª•, qu·∫£n l√Ω, ...\n",
    "* ƒê∆∞·ª£c ph√©p sinh ra nh·ªØng c√¢u h·ªèi kh√¥ng c√≥ trong lu·∫≠t (VD: \"Lu·∫≠t c√≥ quy ƒë·ªãnh v·ªÅ h·ªçc online kh√¥ng?\").\n",
    "* M·ªói c√¢u h·ªèi ch·ªâ n√™n t·∫≠p trung v√†o m·ªôt n·ªôi dung, nh∆∞ng ‚â§10% multi-intent.\n",
    "* Vi·∫øt b·∫±ng ng√¥n ng·ªØ t·ª± nhi√™n.\n",
    "\n",
    "### B∆∞·ªõc 2. Sinh c√¢u tr·∫£ l·ªùi:\n",
    "* R√µ r√†ng, d·ª©t kho√°t, trung t√≠nh, kh√¥ng ƒë·∫°i t·ª´ nh√¢n x∆∞ng.\n",
    "* N·∫øu thi·∫øu th√¥ng tin, tag insufficient_context: true (‚â§10%).\n",
    "\n",
    "### B∆∞·ªõc 3. Tr√≠ch xu·∫•t tham chi·∫øu:\n",
    "* ƒêo·∫°n vƒÉn b·∫£n ƒë·∫ßy ƒë·ªß, li·ªÅn m·∫°ch (to√†n b·ªô b·∫£ng/danh s√°ch n·∫øu c√≥), 400-1300 token.\n",
    "\n",
    "### B∆∞·ªõc 4. G√°n c·∫•p ƒë·ªô suy di·ªÖn:\n",
    "0 ‚Äì Kh√¥ng c·∫ßn (~25%): Tra c·ª©u tr·ª±c ti·∫øp.\n",
    "1 ‚Äì Nh·∫π (~30%): 2‚Äì3 b∆∞·ªõc.\n",
    "2 ‚Äì ƒêa b∆∞·ªõc (~45%): >3 b∆∞·ªõc ph·ª©c t·∫°p.\n",
    "\n",
    "### B∆∞·ªõc 5. G√°n lo·∫°i c√¢u h·ªèi:\n",
    "Khi n√†o, ·ªû ƒë√¢u, Ai, C√°i g√¨, Bao nhi√™u, Nh∆∞ th·∫ø n√†o.\n",
    "\n",
    "## ƒê·∫ßu ra:\n",
    "List JSON objects:\n",
    "[{{\n",
    "  \"id\": \"<m√£ duy nh·∫•t>\",\n",
    "  \"question\": \"<c√¢u h·ªèi>\",\n",
    "  \"answer\": \"<tr·∫£ l·ªùi>\",\n",
    "  \"reference\": \"<tham chi·∫øu>\",\n",
    "  \"multi_intent\": \"<true|false>\",\n",
    "  \"insufficient_context\": \"<true|false>\",\n",
    "  \"reasoning_level\": \"<0|1|2>\"\n",
    "  \"topic\": \"<ch·ªß ƒë·ªÅ>\",\n",
    "  \"question_type\": \"<lo·∫°i>\"\n",
    "}}]\n",
    "\n",
    "N·ªôi dung lu·∫≠t: {law_text}\n",
    "\"\"\"\n",
    "\n",
    "def call_gemini_api(prompt: str) -> str:\n",
    "    \n",
    "    print(\"--- B·∫Øt ƒë·∫ßu g·ªçi Gemini API... ---\")\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                candidate_count=1,\n",
    "                temperature=0.0,\n",
    "                response_mime_type=\"application/json\",\n",
    "            ),\n",
    "        )\n",
    "        response_text = response.text\n",
    "        \n",
    "        # X·ª≠ l√Ω JSON\n",
    "        response_text = re.sub(r\"^```json\\n\", \"\", response_text)\n",
    "        response_text = re.sub(r\"\\n```$\", \"\", response_text)\n",
    "        print(\"--- G·ªçi API th√†nh c√¥ng. ---\")\n",
    "        return response_text\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªñI khi g·ªçi Gemini API: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5c3a7a",
   "metadata": {},
   "source": [
    "Th·ª±c hi·ªán qu√° tr√¨nh t·∫£i d·ªØ li·ªáu, sinh prompt, g·ªçi API v√† l∆∞u k·∫øt qu·∫£ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b17003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang t·∫£i vƒÉn b·∫£n lu·∫≠t t·ª´: ../inputs/luat_giao_duc_input_converter.json\n",
      "T·∫£i vƒÉn b·∫£n lu·∫≠t th√†nh c√¥ng.\n",
      "\n",
      "--- ƒêang sinh d·ªØ li·ªáu cho ch·ªß ƒë·ªÅ: QuyDinhChung ---\n",
      "--- B·∫Øt ƒë·∫ßu g·ªçi Gemini API... ---\n",
      "--- G·ªçi API th√†nh c√¥ng. ---\n",
      "ƒê√£ sinh th√†nh c√¥ng 4 c√¢u h·ªèi cho ch·ªß ƒë·ªÅ: QuyDinhChung\n",
      "\n",
      "--- ƒêang sinh d·ªØ li·ªáu cho ch·ªß ƒë·ªÅ: MucTieuGiaoDuc ---\n",
      "--- B·∫Øt ƒë·∫ßu g·ªçi Gemini API... ---\n",
      "--- G·ªçi API th√†nh c√¥ng. ---\n",
      "ƒê√£ sinh th√†nh c√¥ng 4 c√¢u h·ªèi cho ch·ªß ƒë·ªÅ: MucTieuGiaoDuc\n",
      "\n",
      "--- ƒêang sinh d·ªØ li·ªáu cho ch·ªß ƒë·ªÅ: HeThongGiaoDucQuocDan ---\n",
      "--- B·∫Øt ƒë·∫ßu g·ªçi Gemini API... ---\n",
      "--- G·ªçi API th√†nh c√¥ng. ---\n",
      "ƒê√£ sinh th√†nh c√¥ng 4 c√¢u h·ªèi cho ch·ªß ƒë·ªÅ: HeThongGiaoDucQuocDan\n",
      "\n",
      "--- ƒêang sinh d·ªØ li·ªáu cho ch·ªß ƒë·ªÅ: ChuongTrinhGiaoDuc ---\n",
      "--- B·∫Øt ƒë·∫ßu g·ªçi Gemini API... ---\n",
      "--- G·ªçi API th√†nh c√¥ng. ---\n",
      "ƒê√£ sinh th√†nh c√¥ng 4 c√¢u h·ªèi cho ch·ªß ƒë·ªÅ: ChuongTrinhGiaoDuc\n",
      "\n",
      "--- ƒêang sinh d·ªØ li·ªáu cho ch·ªß ƒë·ªÅ: QuanLyNhaNuocVeGiaoDuc ---\n",
      "--- B·∫Øt ƒë·∫ßu g·ªçi Gemini API... ---\n",
      "--- G·ªçi API th√†nh c√¥ng. ---\n",
      "ƒê√£ sinh th√†nh c√¥ng 4 c√¢u h·ªèi cho ch·ªß ƒë·ªÅ: QuanLyNhaNuocVeGiaoDuc\n",
      "\n",
      "üéâ Ho√†n t·∫•t! ƒê√£ l∆∞u 20 m·∫´u test v√†o file: test_set_giao_duc.json\n"
     ]
    }
   ],
   "source": [
    "def main_generation_process():\n",
    "    \n",
    "    LAW_FILE_PATH = \"../inputs/luat_giao_duc_input_converter.json\" \n",
    "    \n",
    "    # T√™n file JSON k·∫øt qu·∫£ ƒë·∫ßu ra\n",
    "    OUTPUT_FILE_PATH = \"test_set_giao_duc.json\"\n",
    "    \n",
    "    # S·ªë l∆∞·ª£ng c√¢u h·ªèi sinh cho M·ªñI ch·ªß ƒë·ªÅ\n",
    "    QUESTIONS_PER_TOPIC = 4\n",
    "    \n",
    "    # C√°c ch·ªß ƒë·ªÅ cho Lu·∫≠t Gi√°o d·ª•c\n",
    "    law_topics = [\n",
    "        \"QuyDinhChung\",\n",
    "        \"MucTieuGiaoDuc\",\n",
    "        \"HeThongGiaoDucQuocDan\",\n",
    "        \"ChuongTrinhGiaoDuc\",\n",
    "        \"QuanLyNhaNuocVeGiaoDuc\"\n",
    "    ]\n",
    "\n",
    "    # T·∫£i vƒÉn b·∫£n lu·∫≠t t·ª´ JSON\n",
    "    print(f\"ƒêang t·∫£i vƒÉn b·∫£n lu·∫≠t t·ª´: {LAW_FILE_PATH}\")\n",
    "    law_text = load_law_document(LAW_FILE_PATH)\n",
    "    if not law_text:\n",
    "        return\n",
    "    print(\"T·∫£i vƒÉn b·∫£n lu·∫≠t th√†nh c√¥ng.\")\n",
    "\n",
    "    all_test_data = []\n",
    "    \n",
    "    # L·∫∑p qua t·ª´ng ch·ªß ƒë·ªÅ ƒë·ªÉ sinh d·ªØ li·ªáu\n",
    "    for topic in law_topics:\n",
    "        print(f\"\\n--- ƒêang sinh d·ªØ li·ªáu cho ch·ªß ƒë·ªÅ: {topic} ---\")\n",
    "        \n",
    "        # X√¢y d·ª±ng prompt\n",
    "        prompt = build_generation_prompt(topic, law_text, QUESTIONS_PER_TOPIC)\n",
    "        \n",
    "        # G·ªçi Gemini API\n",
    "        response_text = call_gemini_api(prompt)\n",
    "        \n",
    "        if not response_text:\n",
    "            print(f\"L·ªñI: Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi cho ch·ªß ƒë·ªÅ: {topic}\")\n",
    "            continue\n",
    "        \n",
    "        # X·ª≠ l√Ω k·∫øt qu·∫£ JSON\n",
    "        try:\n",
    "            generated_data = json.loads(response_text)\n",
    "            if isinstance(generated_data, list):\n",
    "                print(f\"ƒê√£ sinh th√†nh c√¥ng {len(generated_data)} c√¢u h·ªèi cho ch·ªß ƒë·ªÅ: {topic}\")\n",
    "                all_test_data.extend(generated_data)\n",
    "            else:\n",
    "                print(f\"L·ªñI: API kh√¥ng tr·∫£ v·ªÅ m·ªôt danh s√°ch JSON. ƒê√£ nh·∫≠n: {type(generated_data)}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"L·ªñI: Kh√¥ng th·ªÉ parse JSON. L·ªói: {e}\")\n",
    "            print(f\"D·ªØ li·ªáu th√¥ nh·∫≠n ƒë∆∞·ª£c:\\n{response_text}\")\n",
    "\n",
    "    # L∆∞u k·∫øt qu·∫£\n",
    "    if all_test_data:\n",
    "        try:\n",
    "            with open(OUTPUT_FILE_PATH, 'w', encoding='utf-8') as f:\n",
    "                json.dump(all_test_data, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"\\nüéâ Ho√†n t·∫•t! ƒê√£ l∆∞u {len(all_test_data)} m·∫´u test v√†o file: {OUTPUT_FILE_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"L·ªñI khi l∆∞u file: {e}\")\n",
    "    else:\n",
    "        print(\"\\nKh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c sinh ra. Vui l√≤ng ki·ªÉm tra l·∫°i.\")\n",
    "\n",
    "main_generation_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab8ccc7",
   "metadata": {},
   "source": [
    "L∆∞u k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbfde957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'test_set_giao_duc.json' ch∆∞a ƒë∆∞·ª£c t·∫°o. H√£y ch·∫°y cell 'main_generation_process' tr∆∞·ªõc.\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_FILE_PATH = \"test_set_giao_duc.json\"\n",
    "\n",
    "try:\n",
    "    with open(OUTPUT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"N·ªôi dung file '{OUTPUT_FILE_PATH}':\\n\")\n",
    "    print(json.dumps(data, indent=2, ensure_ascii=False))\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{OUTPUT_FILE_PATH}' ch∆∞a ƒë∆∞·ª£c t·∫°o. H√£y ch·∫°y cell 'main_generation_process' tr∆∞·ªõc.\")\n",
    "except Exception as e:\n",
    "    print(f\"L·ªói khi ƒë·ªçc file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
